<!DOCTYPE html>
<!--  This site was created in Webflow. http://www.webflow.com  -->
<!--  Last Published: Fri Sep 27 2019 01:54:12 GMT+0000 (UTC)  -->
<html data-wf-page="5d8bdf83d9987b66d7875440" data-wf-site="5cbca073fad96c5a1399bd63">
<head>
  <meta charset="utf-8">
  <title>VideoThat</title>
  <meta content="We create magic. We make people move." name="description">
  <meta content="VideoThat" property="og:title">
  <meta content="We create magic. We make people move." property="og:description">
  <meta content="https://uploads-ssl.webflow.com/5cbca073fad96c5a1399bd63/5cc99609d546877df176e2e0_people_move.jpg" property="og:image">
  <meta content="summary" name="twitter:card">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="../css/normalize.css" rel="stylesheet" type="text/css">
  <link href="../css/webflow.css" rel="stylesheet" type="text/css">
  <link href="../css/orians-first-project-b3c5bf.webflow.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Josefin Sans:regular,italic","Josefin Sans:regular,700"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="../images/favicon.png" rel="shortcut icon" type="image/x-icon">
  <link href="../images/webclip.png" rel="apple-touch-icon">
  <!--  Google Analytics  -->
  <script>	
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;	
ga('create', 'UA-132702233-1', 'auto');	
ga('send', 'pageview');	
</script>
  <script async="" src="https://www.google-analytics.com/analytics.js"></script>
  <!--  End Google Analytics  -->
</head>
<body>
  <div class="div-block-4">
    <div data-collapse="medium" data-animation="default" data-duration="400" class="navigation w-nav">
      <div class="navigation-items"><a href="../index.html" class="logo-link w-nav-brand"><img src="../images/meimadix_logo_white_on_transparent.png" width="200" srcset="../images/meimadix_logo_white_on_transparent-p-500.png 500w, ../images/meimadix_logo_white_on_transparent-p-800.png 800w, ../images/meimadix_logo_white_on_transparent.png 960w" sizes="(max-width: 479px) 75vw, 200px" alt=""></a>
        <div class="navigation-wrap">
          <nav role="navigation" class="navigation-items w-nav-menu"><a href="../index.html" class="navigation-item w-nav-link">Home</a><a href="../posts/videothat.html" class="navigation-item blog-link w-nav-link w--current">Blog</a></nav>
          <div class="menu-button w-nav-button"><img src="../images/menu-icon_1menu-icon.png" width="22" alt="" class="menu-icon"></div>
        </div>
      </div>
    </div>
  </div>
  <div>
    <div></div>
    <div class="w-container">
      <article>
        <h4>Blog</h4>
        <div class="blog-post w-richtext">
          <h1>VideoThat!</h1>
          <h3>Automagically Generate a Music Video for Your Input Audio</h3>
          <h4>2nd place DataHack winner 🥈, September 2019</h4>
          <p>By Dalya Gartzman, Orian Sharoni, Yaara Arkin, Yael Daihes, Roee Shenberg</p>
          <p><a href="https://github.com/Sharonio/VideoThat" target="_blank">VideoThat</a> will automagically generate the perfect video compilation that fits impeccably to the theme song of your choice. The algorithm’s default choice is to create a video clip from Taylor Swift’s videos to an Arctic Monkeys song.</p>
        </div>
        <div class="html-embed w-embed"><video src="https://www.meimadix.com/videos/videoThat_snippet.mp4" controls="" style="display:inline-block;max-width:100%;max-height:300px;"></video></div>
        <section class="blog-post w-richtext">
          <h3>Every good algorithm needs a use case</h3>
          <p>It’s your best friend’s wedding, and you want to make something nice for them. You gathered all of your mutual friends and recorded a personal song full of inside jokes and memorable stories. Everything is almost ready! Now all you have to do is manually edit all the party videos you have to fit perfectly to the song!</p>
          <p>Say what now??<br></p>
          <p>Yup, sounds like a rather tedious task… Well, luckily, no need to do it manually anymore!</p>
          <p><a href="https://github.com/Sharonio/VideoThat">VideoThat</a> to the rescue!<br></p>
          <figure class="w-richtext-align-center w-richtext-figure-type-image">
            <div><img src="../images/HoKLjMA2aS2hQEALxbnoZCJ0sxzyQuUXJBxn-H11fZuR7TeXatUYOVhXRLriHOJMYDSWP550gtnsKZYEBPEZMgv-QloXFNpvVIGngrEg6LTuDjcp6kyLEh0gEAE3j6jAP0gjZjGC.png" alt=""></div>
          </figure>
          <h3>The premise</h3>
          <p>The underlying assumption for this project is that the input videos already have an audio that fits perfectly to them. Therefore, we can match between video segments and audio snippets from your theme song, by finding correlated audio patterns.<br></p>
          <figure class="w-richtext-align-center w-richtext-figure-type-image">
            <div><img src="../images/FHANO1MP3CvofFnCDKY23-qPcNcAFUCDZB_kG5ipt8-Q4bhnbuA4Ecgv7WyHZsbAibv2Frl9fVs7l9PUjs61WyhvGp1ZJm9H31BBbfICv7WPE7AL8iVkWdzj4NNJOyOEWUaDiTef.png" alt=""></div>
          </figure>
          <h3>Beat Feature Extraction: Spectral Novelty</h3>
          <p>Our premise, then, is that matching the beat between the source material and the target song will yield a video that works well with the target song. The question is how to define beat matching, and our choice is to try to align as many note starts as possible.<br></p>
          <p>When a note is played, intuitively, we’re adding sound. The naïve way to quantify this is to look at the amount of energy (RMSE) added to the signal. Or, in algorithmic terms, calculate the energy in short windows of time, then calculate the difference between the amounts of energy (delta RMSE), and only keep positive values as we’re looking for note starts (energy novelty).<br></p>
          <figure class="w-richtext-align-center w-richtext-figure-type-image">
            <div><img src="../images/EzxZT4SIk0nNk4ZN-AeDBlPrZdnUoa8lFrRAB91RYl9iMbdwhElU9MW4FiaYgxuQY1UoXj3e9ash0rj-wTcpqlbZ4I9ooeD0iZuZYxD929xdS4bzy__eREbzxMuU_yeBKvseGMwX.png" alt=""></div>
            <figcaption>Audio and its corresponding RMSE, delta-RMSE and energy-novelty</figcaption>
          </figure>
          <p>This works well if we only have one instrument playing one note at a time. Fortunately, that’s not usually the case in real music, so we extend the principle, by looking at energy in different frequency bands: if we play a new note while the old one is still fading away, we might be keeping the total amount of energy in the song the same, but the new note is higher or lower in frequency, so we’ll be able to detect the energy being added now.<br></p>
          <p>Librosa wraps this logic into <a href="http://librosa.github.io/librosa/generated/librosa.onset.onset_strength.html">librosa.onset.onset_strength</a>, which yields a good signal for the task of detecting note onsets. If you want to be fun at parties, you can describe what it does as computing half-wave rectified spectral flux.</p>
          <p>More resources: <a href="https://musicinformationretrieval.com/novelty_functions.html">Reference with code examples</a>, <a href="https://librosa.github.io/librosa/_modules/librosa/onset.html#onset_strength_multi">The source code for onset_strength</a></p>
          <h3>The pipeline<br></h3>
          <p>Before we can match between input videos segments and audio segments, we would first need to find those segments...</p>
          <p>Segmenting videos can be done automatically, using the <a href="https://pypi.org/project/scenedetect/">scenedetect</a> python package. <br></p>
          <p>Then, to generate a fingerprint for each audio segment at hand, we use <a href="https://librosa.github.io/librosa/">librosa</a> python package to discover <a href="https://librosa.github.io/librosa/generated/librosa.onset.onset_detect.html#librosa.onset.onset_detect">onset</a> patterns.<br></p>
          <figure class="w-richtext-align-center w-richtext-figure-type-image">
            <div><img src="../images/onsets_strength_smaller.png" alt=""></div>
            <figcaption>Spectral Novelty</figcaption>
          </figure>
          <h3>Scoring Function or: how to best match sound with scenes<br></h3>
          <p>Finally, working in a greedy manner, we match video segments to the theme song, by finding the unused video who’s audio is best correlated with the next segment in our theme song. To compute the correlation, we used a scoring function.</p>
          <figure class="w-richtext-align-center w-richtext-figure-type-image">
            <div><img src="../images/n5L4mgRCmj5Cg2jUAJuYDyASNaK9L21Av9Fd1SJFpEMHPAYmpWpBojUrTxpFwHgpeprw6rSFeUNEzYPciq-i-FAA42WzBPv68UIhFxN1Z9FctMGHdKzUDakfqhx_V0TOl55ESew2.gif" alt=""></div>
          </figure>
          <p>The scoring function we chose is based on the correlation between the spectral novelty of each clip and the spectral novelty of the target song. However, choosing the raw correlation is a poor choice in practice, because the spectral novelty is a non-negative quantity, which biases us towards longer clips since they have more chances of having notes align even if the beat doesn’t match.<br></p>
          <p>To combat this, we can look at the rate of correlation, or ‘correlation per second’, by dividing the total correlation by the length of the clip we correlate to. This is also not the best, since it biases us towards short clips that align perfectly, while we do want longer matches that aren’t 100% on-beat for a more satisfying result.<br></p>
          <p>In the end we chose to use a balanced approach of dividing the correlation by the square root of the length of the clip, which still biases towards longer clips, but gives short-but-good matches a fighting chance:</p>
          <figure class="w-richtext-align-center w-richtext-figure-type-image">
            <div><img src="../images/a1c_NtKJsxRPz5tmrf7tRytEBAstC0LBDifnVgHRdiOjQWogSKacWRHAhdFvP1ximGBn7fdBw-HqLck-rcf_8TRlBzex1WiIdPFDLq3PfwAoTh1oQez8oewOiNdoPoTP_J6j1slV.png" alt=""></div>
            <figcaption>Our scoring function</figcaption>
          </figure>
          <h3>Alternative Solutions</h3>
          <p>You might ask yourself, why greedy? There has to be a better way to do this!</p>
          <p>And indeed there is. There <strong>are</strong> in fact. Many better ways. As you can read on the next section, this project was tight in time, and quick wins were the expected result. </p>
          <p>But still, we did try some different approaches, and thought of future ones:</p>
          <h4>Smarter search</h4>
          <p>Greedy search makes locally-good decisions that may be globally poor. A simple approach to rectifying this issue is to use a <strong>beam search</strong> in order to take more options into account. Another alternative would be to use a search strategy which is not sequential. For example, try to find globally good matching clips (e.g. &quot;clip 7 matches the middle of the song well&quot;) and then fill in the spaces left.</p>
          <h4>Better score</h4>
          <p>There’s plenty of room to fudge around with the score (e.g. subtract a small amount from the spectral novelty so that a note compared with silence will actively penalize the score, and silence on silence will add to it)</p>
          <h3>Behind the scenes</h3>
          <p>This project was conceived, born and matured, during a 40 hour long Data Science Hackathon called <a href="https://www.datahack.org.il/">DataHack</a> (or as we like to call it in Israel - the yearly field trip of Data Science), held in Jerusalem every year, 2019 being the 5th event 🤓</p>
          <p>Our team was formed out of previously random loose connections, and we were all taking a leap of faith hoping this will be a great match. Luckily, the chemistry was quick to manifest! We had a great time taking this idea from “what are we going to do” to “omg this is going to be amazing” to “ok here are the steps we should take.&#x27;&#x27;</p>
          <p>To demonstrate the <a href="https://github.com/Sharonio/VideoThat">VideoThat</a> ability, we took the top 30 videos of <a href="https://www.youtube.com/taylorswift">Taylor Swift</a> on YouTube to stand for our video data set, and as the theme song we chose “<a href="https://www.youtube.com/watch?v=bpOSxM0rNPM">Do I Wanna Know</a>” by Arctic Monkeys.</p>
          <p>We each learned a lot, enjoyed our time interchanging ideas, held fruitful and insightful conversations, practiced a good work life balance during those 40 hours, and even <a href="https://www.facebook.com/datahackil/photos/a.427954734076246/1196000310605014/?type=3&amp;theater">won second place</a>!</p>
          <figure class="w-richtext-align-center w-richtext-figure-type-image">
            <div><img src="../images/Qvr-y5bvodwMEUM4yVz1u0_r5IfCH0fPDmz3JuJMvplkCNIZXEGDeq9IireS7B-dpzAE-l8ax_MhnPcbra8UCA8w35ieO8-T0IfhdYgWrlj1sKqdhoxxW-cP-V8yvl2sESuDqWzJ.png" alt=""></div>
            <figcaption>Practicing work-life balance at a brunch during the hackathon</figcaption>
          </figure>
          <figure class="w-richtext-align-center w-richtext-figure-type-image">
            <div><img src="../images/HeZbFlwYV6WLA3QBjyNT6nun5eL-ByeJdzpiEpnscO40tTgziLYE8xL9bINWY4NaXbnK_ZLuTBdN1-tjHEapp7MBlu0lIyz7O-TUo7_92VQfxGf2wRuKwG1KsK4LJvfojp9NqCzO.png" alt=""></div>
            <figcaption>Winning second place</figcaption>
          </figure>
          <p>This is us:</p>
          <p>Yael Daihes <a href="https://github.com/yooli3">GitHub</a> <a href="https://www.linkedin.com/in/yael-daihes/">LinkedIn<br></a>Yaara Arkin <a href="https://github.com/yaarasegre">GitHub</a> <a href="https://www.linkedin.com/in/yaara-arkin-86706013/">LinkedIn<br></a>Orian Sharoni <a href="https://github.com/Sharonio">GitHub</a> <a href="https://www.linkedin.com/in/orian-sharoni/">LinkedIn<br></a>Roee Shenberg <a href="https://github.com/shenberg">GitHub</a> <a href="https://www.linkedin.com/in/roeeshenberg/">LinkedIn<br></a>Dalya Gartzman <a href="https://github.com/DalyaG">GitHub</a> <a href="https://www.linkedin.com/in/dalya-gar/">LinkedIn</a><br></p>
        </section>
      </article>
    </div>
  </div>
  <div>
    <div></div>
  </div>
  <div class="footer-wrap">
    <div id="w-node-53c71d954aa6-1d954aa0" class="footer-links"><a href="https://www.facebook.com/meimadix/" target="_blank" class="footer-item">Facebook</a><a href="https://www.instagram.com/meimadix/" target="_blank" class="footer-item">Instagram</a><a href="mailto:art@meimadix.com" class="footer-item">art@meimadix.com<br></a><a href="tel:+972-545-715-309" class="footer-item">+972-545-715-309<br></a></div><img src="../images/meimadix_logo_1-copy.jpg" width="200" srcset="../images/meimadix_logo_1-copy-p-500.jpeg 500w, ../images/meimadix_logo_1-copy.jpg 948w" sizes="(max-width: 479px) 100vw, (max-width: 991px) 200px, 13vw" id="w-node-f55f64e8020d-1d954aa0" alt="" class="image"></div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.4.1.min.220afd743d.js" type="text/javascript" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
  <script src="../js/webflow.js" type="text/javascript"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
</body>
</html>